{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1P60ucf3WnwxP7ggumxfhqBYm1QPL0Gcm","authorship_tag":"ABX9TyOfQDDQe0RvCagTO6WCgZem"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mKa57FYwytQw","executionInfo":{"status":"ok","timestamp":1675520951868,"user_tz":-60,"elapsed":941,"user":{"displayName":"Jiahao Zhang","userId":"01900003456347215448"}},"outputId":"0885c41d-7f62-40a5-9fd0-0ac3d11a2fb4"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1Q-s7iBg9LuhiMq6j_d9cgyLVr2MFTUz_/Applied_Data_Science_Project/Deliverables/Code/final_GPT-2\n"]}],"source":["cd \"/content/drive/MyDrive/Applied_Data_Science_Project/Deliverables/Code/final_GPT-2\""]},{"cell_type":"code","source":["!pip install transformers\n","!pip install syllables\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"70w0bFKT0cUv","executionInfo":{"status":"ok","timestamp":1675521835448,"user_tz":-60,"elapsed":8745,"user":{"displayName":"Jiahao Zhang","userId":"01900003456347215448"}},"outputId":"40892727-a1b9-49b4-9824-ea33b4280e2b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting syllables\n","  Downloading syllables-1.0.6-py3-none-any.whl (15 kB)\n","Collecting importlib-metadata<6.0.0,>=5.1.0\n","  Downloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\n","Collecting cmudict<2.0.0,>=1.0.11\n","  Downloading cmudict-1.0.13-py3-none-any.whl (939 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.3/939.3 KB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: importlib-resources<6.0.0,>=5.10.1 in /usr/local/lib/python3.8/dist-packages (from cmudict<2.0.0,>=1.0.11->syllables) (5.10.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata<6.0.0,>=5.1.0->syllables) (3.12.0)\n","Installing collected packages: importlib-metadata, cmudict, syllables\n","  Attempting uninstall: importlib-metadata\n","    Found existing installation: importlib-metadata 6.0.0\n","    Uninstalling importlib-metadata-6.0.0:\n","      Successfully uninstalled importlib-metadata-6.0.0\n","Successfully installed cmudict-1.0.13 importlib-metadata-5.2.0 syllables-1.0.6\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["!python train.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ggPiNrJd0V9J","executionInfo":{"status":"ok","timestamp":1675521755034,"user_tz":-60,"elapsed":311199,"user":{"displayName":"Jiahao Zhang","userId":"01900003456347215448"}},"outputId":"77e735d8-bd70-413e-cb04-2b51eaeb43d3"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting...\n","0\n","Starting...\n","0\n","Starting...\n","0\n","(14703, 3)\n","(1307, 3)\n","(327, 3)\n","Downloading (…)lve/main/config.json: 100% 718/718 [00:00<00:00, 286kB/s]\n","Downloading (…)olve/main/vocab.json: 100% 1.04M/1.04M [00:00<00:00, 6.35MB/s]\n","Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 3.44MB/s]\n","Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 8.40MB/s]\n","Downloading (…)\"pytorch_model.bin\";: 100% 1.52G/1.52G [00:08<00:00, 183MB/s]\n","Downloading (…)neration_config.json: 100% 124/124 [00:00<00:00, 46.9kB/s]\n","/content/drive/.shortcut-targets-by-id/1Q-s7iBg9LuhiMq6j_d9cgyLVr2MFTUz_/Applied_Data_Science_Project/Deliverables/Code/final_GPT-2/utils_train.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n","/content/drive/.shortcut-targets-by-id/1Q-s7iBg9LuhiMq6j_d9cgyLVr2MFTUz_/Applied_Data_Science_Project/Deliverables/Code/final_GPT-2/utils_train.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n","Using cuda_amp half precision backend\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 14704\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 18380\n","  Number of trainable parameters = 354834432\n","  0% 0/18380 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Traceback (most recent call last):\n","  File \"train.py\", line 64, in <module>\n","    trainer = myTrainer(train_dataset, valid_dataset, 'output', epochs, batch_size, tokenizer, model)\n","  File \"/content/drive/.shortcut-targets-by-id/1Q-s7iBg9LuhiMq6j_d9cgyLVr2MFTUz_/Applied_Data_Science_Project/Deliverables/Code/final_GPT-2/utils_train.py\", line 93, in myTrainer\n","    trainer.train()\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 1543, in train\n","    return inner_training_loop(\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 1791, in _inner_training_loop\n","    tr_loss_step = self.training_step(model, inputs)\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 2539, in training_step\n","    loss = self.compute_loss(model, inputs)\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 2571, in compute_loss\n","    outputs = model(**inputs)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1043, in forward\n","    transformer_outputs = self.transformer(\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 887, in forward\n","    outputs = block(\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 425, in forward\n","    feed_forward_hidden_states = self.mlp(hidden_states)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 353, in forward\n","    hidden_states = self.act(hidden_states)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/activations.py\", line 35, in forward\n","    return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 14.76 GiB total capacity; 13.71 GiB already allocated; 11.88 MiB free; 13.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","  0% 0/18380 [00:03<?, ?it/s]\n"]}]},{"cell_type":"code","source":["!python evaluate.py"],"metadata":{"id":"X2EhMSya4Z8y"},"execution_count":null,"outputs":[]}]}